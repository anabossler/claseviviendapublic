# -*- coding: utf-8 -*-
"""Berlin-machine learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uPD0E2bTrc5QfJ3vNiDEuXJXIio4KgUr

# Explorando el mercado de alquiler de corto plazo de Berlin 
## Predicción de precios desde AirBnB con machine learning
*Palabras-clave: Airbnb, Berlin,data science, geopandas, geospatial, matplotlib

Alumno: Ana de Souza Bossler
Asignatura: Economía de la Vivienda

### 1. Antecedentes del proyecto y objetivos

El Airbnb es una plataforma de alquiler de corto plazo. Permite a los hosts alquilar casas enteras o habitaciones libres por períodos cortos de tiempo. Uno de los desafíos que se enfrentan es determinar el precio óptimo por noche.   Dado que AirBnB es un market place, sus precios están conectados a los precios de mercado. Hay diferentes criterios para la determinación de estos precios: zona de ubicación del inmueble, ocupación promedio, existencia de amenities cerca, número de habitaciones y otras características del inmueble, entre otros.
 
Pese que AirBnB guía los hosts en la determinación de precios, los critérios de determinación no están claros para los hosts.

Como criterio general, los hosts pueden buscar listings que son similares por zona y otras característiacs dedl inmueble y hacer un promedio de los precios para determinar lo suyo, pero dado que el mercado es dinámico, el host tendría que actualizar los precios regularmente.

Del punto de vista económico esto tampoco es muy acertado, dado que podemos dejar de considerar factores que puedan darnos ventajas comparativas como servicios extras que el inmueble pueda ofrecer. 

Por ello, un modelo de precios hedónicos se hace necesario. Los precios hedónicos están conectados con la utilidad percibida, donde las variables independientes son las que se refieren a la calidad de un inmueble y la dependiente es el precio. A pesar de que se puede calcular el impacto de dichos factores en los precios utilizando una regresión lineal, la capacidad de predición es limitada. 

El objetivo de este trabajo es aplicar machine learning supervisado para predecir los precios de alquiler de corto plazo en Berlin. 

En relación a la diferencia del proceso estadístico "tradicional", esta se da porque en el modelo estadístico debe encontrarse una línea que minimiza el error medio al cuadrado en todos los datos, suponiendo que tengan una relación lineal, y, un ruido aleatorio que suele ser  gaussiano. El objetivo del modelo es determinar la relación entre los datos y nuestra variable dependiente, no hacer predicciones sobre datos futuros, o sea, una inferencia estadística, diferente de la predicción. 

Sin embargo, todavía podemos usar este modelo para hacer predicciones, pero la forma en que se evalúa el modelo implicará evaluar la importancia y la solidez de los parámetros del modelo.

Ya en en el caso del machine learning supervisado es obtener un modelo que pueda hacer predicciones repetibles, aunque no importa si el modelo es interpretable dado que el machine learning se relaciona más con los resultados.

Para este trabajo utilizaremos un predictor basado en el espacio: la proximidad de la propiedad en relación a ciertos venues, posibilitando que se pueda poner un precio a características como vivir cerca de una cafetería o un centro comercial. El trabajo está basado en el modelo de precios hedónicos de Graciela Carriló aplicado al mercado de alquiler de corto plazo de Edinburgo, con adaptaciones importantes hechas por la alumna en el código Python, dado la imposibilidad de conectar con Foursquare (límite de 950 casos por día), y, de utilizar pandanas para la determinación de la correlación espacial. Se ha utilizado scrapy para recoger la información sobre los venues de Berlin.

### 2. Descripción de los Datos

Airbnb no publica ningún dato oficial, pero un grupo separado llamado Inside Airbnb recopila información disponible públicamente sobre los listados de Airbnb utilizando scrapy. Utilizaremos los datos de esta plataforma para este proyecto, recogidos en 17 de marzo de 2020 en la ciudad de Berlin, Alemania. Contiene información sobre todos los listados de AirBnB Berlin que estaban publicados en su página web en esa fecha (25207 inputs)

Los datos no reflejan los precios reales pagados por los consumidores, sino los precios anunciados en AirBnB.

Cada línea se refiere a una observación de un inmueble publicitado en la página web y las columnas describen diferentes características del listing. Hemos añadido 6 columnas para el análisis: latitud del centro de Berlin (determinado como Mitte), longitud del centro de Berlin, distancia del inmueble a este centro, y dada las características turísticas del alquiler de corto plazo hemos añadido la latitud y la longitud de Alexanderplatz y la distancia del inmueble a Alexanderplatz.

Utilizaremos las siguientes de la base de datos de AirBnB.

- `accommodates`: número de huespedes que puede acomodar
- `bedrooms`: número de habitaciones
- `bathrooms`: número de aseos
- `beds`: número de camas
- `price`: precio por noche
- `minimum_nights`: estadía mínima
- `maximum_nights`: estadía máxima
- `number_of_reviews`: número de reviews que han dejado los huspedes sobre el inmueble
- `availability_365`: número de días disponible en un año
- `latitude`: latitud del inmueble
- `longitude`: longitud del inmueble

#### Instalación de los paquetes requeridos
"""

# paquetes generales
# %load basic_installs.py
import sys
!{sys.executable} -m pip install geopandas==0.6.3
!{sys.executable} -m pip install mplleaflet
!{sys.executable} -m pip install bs4
!{sys.executable} -m pip install geocoder
!{sys.executable} -m pip install geopy
!{sys.executable} -m pip install folium
!{sys.executable} -m pip install lxml
!{sys.executable} -m pip install pygeoj
!{sys.executable} -m pip install pyshp
!{sys.executable} -m pip install datetime
!{sys.executable} -m pip install seaborn
!{sys.executable} -m pip install --upgrade cython

# para series temporales
!{sys.executable} -m pip install statsmodels

# required packages for neighbourhood analysis
!{sys.executable} -m pip install descartes
!{sys.executable} -m pip install requests

# requiered packages for accessibility analysis
# Make sure Cython is upgraded FIRST!
!{sys.executable} -m pip install pandana

# required packages for modelling
!{sys.executable} -m pip install xgboost

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

# general
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Geographical analysis
import geopandas as gpf #libspatialindex nees to be installed first
import json # library to handle JSON files
from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe
import shapefile as shp
import datetime
from statsmodels.tsa.seasonal import seasonal_decompose
import requests
import descartes

# accessibility analysis
import time
from pandana.loaders import osm
from pandana.loaders import pandash5

# modelling
from sklearn.preprocessing import StandardScaler, MinMaxScaler 
from sklearn.model_selection import train_test_split, cross_val_score 
from sklearn.linear_model import LinearRegression
from sklearn import linear_model
from sklearn import metrics
from sklearn import preprocessing 
import xgboost as xgb
from xgboost import plot_importance
from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score

#Hide warnings
import warnings
warnings.filterwarnings('ignore')

# Set plot preference
plt.style.use(style='ggplot')
plt.rcParams['figure.figsize'] = (10, 6)
# %matplotlib inline

print('Libraries imported.')

"""### 3. Limpieza y pre-procesamiento de los datos

Nos hemos basado en este notebook](https://nbviewer.jupyter.org/github/L-Lewis/Airbnb-neural-network-price-prediction/blob/master/Airbnb-price-prediction.ipynb#Categorical-features).
"""

pip install xlrd

from google.colab import drive 
drive.mount('/content/gdrive')

from google.colab import files
uploaded = files.upload()

#!pwd to check the working directory
raw_df = pd.read_csv('listings.csv')
print(f"The dataset contains {len(raw_df)} Airbnb listings")
pd.set_option('display.max_columns', len(raw_df.columns)) # To view all columns
pd.set_option('display.max_rows', 100)
raw_df.head(3)

"""#### Eliminar las columnas iniciales


No utilizaremos lenguaje natural por lo que vamos quitar las columnas de texto libre al igual que otras columnas que no son útiles para predecir el precio (por ejemplo, url, nombre de host y otras características relacionadas con el host que no están relacionadas con la propiedad).
"""

cols_to_drop = ['name', 'host_id', 'host_name','neighbourhood_group',	'neighbourhood']
df = raw_df.drop(cols_to_drop, axis=1)

"""Columns with several `NULL` entries are dropped too."""

df.isna().sum()

df.drop(['last_review', 'reviews_per_month'], axis=1, inplace=True)
df.set_index('id', inplace=True) # ID as index

# Used Label Encoder to convert the categorical columns to numerical.
cols_to_encode = ['room_type']

for col in cols_to_encode:
  label_encoder = preprocessing.LabelEncoder() 
  # Encode labels in column 'species'. 
  df[col]= label_encoder.fit_transform(df[col])

df.columns

df.drop(['latitude', 'longitude'], axis=1, inplace=True)

"""#### Descripción de las variables:

- `room_type` - si son casas completas, habitaciones o habitaciones compartidas

- `price` - precio por noche (variable dependiente)

- `minimum_nights` - estadía mínima

- `availability_365` - cuantas noches están disponibles en un período de un año

- `number_of_reviews` - número de reviews el inmueble tiene

- `last_review` - fecha de la última review

### 4. Análisis Exploratorio

#### Componentes númericos
"""

df.describe()

"""`price`

En cuanto a la distribución de precios, los precios anunciados varían de EUR 0 a EUR 9,000. Creyemos que los extremos del rango se deben a que los anfitriones no entienden cómo usar los precios anunciados de Airbnb correctamente dado que los precios anunciados se pueden establecer en cualquier cantidad arbitraria, y estos son los precios que se muestran cuando las fechas no se ingresan en el sitio. Una vez que ingrese las fechas en que desea ocupar la propiedad, los precios pueden variar mucho.

Como este modelo quiere predecir los precios anunciados en lugar de los precios que realmente se pagaron. No obstante, se realizará una limpieza de los valores particularmente inútiles. Los valores muy pequeños por debajo de EUR 10 se incrementarán a EUR 10.

Hay caídas notables en los precios a EUR 200 (primer gráfico, línea roja), EUR 500 (segundo gráfico, línea verde) y EUR 1,000 (segundo gráfico, línea roja). Los valores superiores a EUR 1,000 se reducirán a EUR 1,000.
"""

print(f"Advertised prices range from £{min(df.price)} to £{max(df.price)}.")

# Distribution of prices from £0 to £1000
plt.figure(figsize=(20,4))
df.price.hist(bins=100, range=(0,1000))
plt.margins(x=0)
plt.axvline(200, color='red', linestyle='--')
plt.title("Precios anunciados en AirBnB hasta los 1000 EUR", fontsize=16)
plt.xlabel("Price (£)")
plt.ylabel("Number of listings")
plt.show()

# Distribution of prices from £200 upwards
plt.figure(figsize=(20,4))
df.price.hist(bins=100, range=(200, max(df.price)))
plt.margins(x=0)
plt.axvline(500, color='green', linestyle='--')
plt.axvline(1000, color='red', linestyle='--')
plt.title("Precios anunciados en AirBnB Berlin", fontsize=16)
plt.xlabel("Price (£)")
plt.ylabel("Number of listings")
plt.show()

# Replacing values under £10 with £10
df.loc[df.price <= 10, 'price'] = 10

# Replacing values over £1000 with £1000
df.loc[df.price >= 1000, 'price'] = 1000

"""**Proximidad con venues**

Como parte de nuestro modelo, buscamos explorar la proximidad a ciertos lugares como un posible predictor de precios. La capacidad de caminar y llegar a lugares puede ser un factor decisivo o decisivo a la hora de elegir un alojamiento. La proximidad a ciertos lugares, como las principales atracciones turísticas, restaurantes, cafeterías e incluso tiendas, podría ayudarnos a predecir el precio. Para esto, utilizaremos Foursquare API para explorar los lugares por vecindario. Como se discutió anteriormente, Mitte es el área que concentra la mayoría de los listados de Airbnb.

Ahora recuperamos la lista de lugares con sus ubicaciones. Usaremos * Foursquare API * para explorar los lugares alrededor de los listados, usando la latitud y longitud de cada vecindario. Luego descubriremos qué lugares son los más comunes y seleccionaremos los lugares más comunes como puntos de interés (POIS) para nuestro análisis de accesibilidad.
"""

# Define Foursquare Credentials 

CLIENT_ID = "TN5TVCGWYYAUSKIHBASFJW1DYKSIWE14OQDSJKOD2OXG2GXV"
CLIENT_SECRET = "ICVRSAJCMQBGYDW0CIPW2VR1SZCEQXSKRKGIISTO0L3IAJZZ"
VERSION = '20190425'
LIMIT = 30
print('Your credentails:')
print('CLIENT_ID: ' + CLIENT_ID)
print('CLIENT_SECRET:' + CLIENT_SECRET)

map_df = pd.DataFrame(columns=['latitude', 'longitude','neighbourhood'], data=raw_df[['latitude','longitude','neighbourhood']].values)
map_df.neighbourhood=map_df.neighbourhood.astype(str)
map_df.head()

# Function to loop for venues through all neighbourhoods
#url = 'https://api.foursquare.com/v2/venues/search?categoryId=50aa9e094b90af0d42d5de0d,530e33ccbcbc57f1066bbff3,530e33ccbcbc57f1066bbff9,4f2a25ac4b909258e854f55f&intent=browse&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format()
def getNearbyVenues(names, latitudes, longitudes, radius):
    venues_list=[]
    count = 0
    for name, lat, lng in zip(names, latitudes, longitudes):
        #print(name)
        
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
        
        try:
          # make the GET request
          results = requests.get(url).json()["response"]['groups'][0]['items']
          #print(results)

          # return only relevant information for each nearby venue
          venues_list.append([(
              name, 
              lat, 
              lng, 
              v['venue']['name'], 
              v['venue']['location']['lat'], 
              v['venue']['location']['lng'],  
              v['venue']['categories'][0]['name']) for v in results])
        except:
            pass
        count = count+1
        if count == 700:
            break
        
    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighbourhood', 
                  'Neighbourhood Latitude', 
                  'Neighbourhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)

"""Now we run the above function on each neighborhood and create a new dataframe called *edinburgh_venues*."""

edinburgh_venues = getNearbyVenues(names=map_df['neighbourhood'],
                                   latitudes=map_df['latitude'],
                                   longitudes=map_df['longitude'],
                                   radius = 500
                                  )

"""Checking the size of the returning dataframe"""

print(edinburgh_venues.shape)
edinburgh_venues.head()

# Saving Data set
edinburgh_venues.to_csv(r'Edinburgh_Venues.csv')

# Read dataset
edinburgh_venues = pd.read_csv('Edinburgh_Venues.csv', index_col=0)

"""Venues returned per neighbourhood"""

edinburgh_venues.groupby('Neighbourhood').count()

"""**Unique categories of venues**"""

print('There are {} unique categories.'.format(len(edinburgh_venues['Venue Category'].unique())))

"""**Number of venues per category**"""

edinburgh_venues.groupby('Venue Category').count()

"""**Analysis per neighbourhood**"""

# One Hot Encoding
edinburgh_onehot = pd.get_dummies(edinburgh_venues[['Venue Category']], prefix = "", prefix_sep = "")

## Add neighbourhood column back to df
edinburgh_onehot['Neighbourhood'] = edinburgh_venues['Neighbourhood']

# Move neighbourhood column to the first column
fixed_columns = [edinburgh_onehot.columns[-1]] + list(edinburgh_onehot.columns[:-1])
edinburgh_onehot = edinburgh_onehot[fixed_columns]

# display
edinburgh_onehot.head()

# New df dimensions
edinburgh_onehot.shape

"""**Group rows by neighbourhood and by taking the mean and the frequency of occurrence of each category**"""

edinburgh_grouped = edinburgh_onehot.groupby('Neighbourhood').mean().reset_index()
edinburgh_grouped

"""**Get each neighbourhood along with its top 5 most common venues**"""

num_top_venues = 5

for hood in edinburgh_grouped['Neighbourhood']:
    print("----"+hood+"----")
    temp = edinburgh_grouped[edinburgh_grouped['Neighbourhood'] == hood].T.reset_index()
    temp.columns = ['venue','freq']
    temp = temp.iloc[1:]
    temp['freq'] = temp['freq'].astype(float)
    temp = temp.round({'freq': 2})
    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))
    print('\n')

"""**Put that in pandas data frame**"""

# Function to sort venues in descending order
def return_most_common_venues(row, num_top_venues):
    row_categories = row.iloc[1:]
    row_categories_sorted = row_categories.sort_values(ascending=False)
    
    return row_categories_sorted.index.values[0:num_top_venues]

import numpy as np

# New dataframe ordered
indicators = ['st', 'nd', 'rd']

# create columns according to number of top venues
columns = ['Neighbourhood']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))

# create a new dataframe
neighbourhoods_venues_sorted = pd.DataFrame(columns=columns)
neighbourhoods_venues_sorted['Neighbourhood'] = edinburgh_grouped['Neighbourhood']

for ind in np.arange(edinburgh_grouped.shape[0]):
    neighbourhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(edinburgh_grouped.iloc[ind, :], 
                                                                          num_top_venues)

neighbourhoods_venues_sorted.head()

"""**Let's look at how many categories are on the 1st most common venue.**"""

print('There are {} unique categories.'.format(len(neighbourhoods_venues_sorted['1st Most Common Venue'].unique())))

neighbourhoods_venues_sorted.groupby('1st Most Common Venue').count()

"""From above, we observe that the most common venues across the dataset are Hotel, Pub, Grocery Store, Supermarket, Cafe, Coffee Shop, with Bar, Bus Stop and Indian Restaurant coming behind. It is clear that different restaurant venues are in subcategories which makes them less common than if they were aggregated. Thus, for the purpose accounting for the venues that may have the most impact on price, we will limit the venues to the most common categories.  It is unlikely that having a hotel nearby will affect price, as Airbnb listings are considered to be on a different category of short-term rental, due to the different benefits they provide versus hotels. Thus, that category of venues will not be considered.

### 5. Preparando los datos
"""

# Open dataset for modelling
df = pd.read_csv(r'listings.csv', index_col=0)
df.head()

columns = ['host_name', 'neighbourhood_group', 'neighbourhood',
       'latitude', 'longitude', 'room_type', 'price', 'minimum_nights',
       'number_of_reviews', 'last_review', 'reviews_per_month',
       'calculated_host_listings_count', 'availability_365']
df = df[columns]

cols_to_encode = []
for col in columns:
  if df[col].dtype == 'object':
    cols_to_encode.append(col)

from sklearn.preprocessing import LabelEncoder
for col in cols_to_encode:
  le = LabelEncoder()
  df[col] = df[col].astype(str)
  df[col] = le.fit_transform(df[col])

"""We now assess for multicollinearity of features:"""

def multi_collinearity_heatmap(df, figsize=(11,9)):
    
    """
    Creates a heatmap of correlations between features in the df. A figure size can optionally be set.
    """
    
    # Set the style of the visualization
    sns.set(style="white")

    # Create a covariance matrix
    corr = df.corr()

    # Generate a mask the size of our covariance matrix
    mask = np.zeros_like(corr, dtype=np.bool)
    mask[np.triu_indices_from(mask)] = True

    # Set up the matplotlib figure
    f, ax = plt.subplots(figsize=figsize)

    # Generate a custom diverging colormap
    cmap = sns.diverging_palette(220, 10, as_cmap=True)

    # Draw the heatmap with the mask and correct aspect ratio
    sns.heatmap(corr, mask=mask, cmap=cmap, center=0, square=True, linewidths=.5, cbar_kws={"shrink": .5}, vmax=corr[corr != 1.0].max().max());

multi_collinearity_heatmap(df, figsize=(20,20))

"""No hay colineralidad expresiva excepto la de num_reviews con reviews_per_month y last_review por lo sólo vamos quitar las reviews per month y last_review"""

# Separating X and y
X = df.drop('price', axis=1)
y = df.price

# Scaling
scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=list(X.columns))
X.fillna(0, inplace=True)

"""### 6. Modelo

Ahora que el procesamiento previo de datos ha terminado, podemos comenzar a aplicar diferentes modelos de Aprendizaje automático supervisado. Compararemos dos modelos:

Un modelo de precios hedónicos espaciales (regresión OLS), con la regresión lineal de la biblioteca Scikit-Learn
El método Gradient Boosting, con XGBRegressor de la biblioteca XGBoost

Las métricas de evaluación utilizadas serán error cuadrático medio (para pérdida) y r cuadrado (para precisión)
"""

# Splitting into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

"""#### Modelo 1: Spatial Hedonic Price Model (HPM)

El modelo hedónico implica la regresión de los precios de venta observados en el listing en comparación con los atributos de una propiedad hipotética que determina el precio de venta. Proviene de la teoría hedónica de precios que supone que una mercancía, como una casa, puede verse como una agregación de componentes o atributos individuales (Griliches, 1971). Se supone que los consumidores compran bienes que incorporan paquetes de atributos que maximizan sus funciones de utilidad subyacentes (Rosen, 1974).

Además de las características de los listados de Airbnb, agregamos características de ubicación, ya que se ha demostrado que son factores importantes para influir en el precio. Idealmente, las pruebas de multiplicador de Lagrange deben realizarse para verificar si hay un retraso espacial en la variable dependiente y, por lo tanto, se prefiere un modelo de retraso espacial para estimar un HPM espacial. Sin embargo, para los fines de este modelo solo estamos utilizando un modelo OLS convencional para la estimación de precios hedónicos que incluye características espaciales y de ubicación, pero no un retraso espacial que explica la dependencia espacial.

Por lo tanto, las primeras variables explicativas son las características de los listados (alojamientos, baños, etc.) y nuestro segundo grupo de variables explicativas basadas en características espaciales y de ubicación son Score, que es la distancia de red al quinto lugar más cercano que calculamos con Pandana; y Barrio perteneciente, 1 si el listado pertenece al barrio especificado, 0 en caso contrario.
"""

hpm_reg_start = time.time()

# Create instance of the model, `LinearRegression` function from 
# Scikit-Learn and fit the model on the training data:

hpm_reg = LinearRegression()  
hpm_reg.fit(X_train, y_train) #training the algorithm

# Now that the model has been fit we can make predictions by calling 
# the predict command. We are making predictions on the testing set:
training_preds_hpm_reg = hpm_reg.predict(X_train)
val_preds_hpm_reg = hpm_reg.predict(X_test)

hpm_reg_end = time.time()

print(f"Time taken to run: {round((hpm_reg_end - hpm_reg_start)/60,1)} minutes")

# Check the predictions against the actual values by using the MSE and R-2 metrics:
print("\nTraining RMSE:", round(mean_squared_error(y_train, training_preds_hpm_reg),4))
print("Validation RMSE:", round(mean_squared_error(y_test, val_preds_hpm_reg),4))
print("\nTraining r2:", round(r2_score(y_train, training_preds_hpm_reg),4))
print("Validation r2:", round(r2_score(y_test, val_preds_hpm_reg),4))

"""Esto significa que nuestras características explican aproximadamente el 2% de la varianza en nuestra variable objetivo.

Interpretar el valor `mean_squared_error` es algo más intuitivo que el valor r cuadrado. El RMSE mide la distancia entre nuestros valores pronosticados y los valores reales.

Podemos comparar los valores de salida reales para `X_test` con los valores pronosticados en un marco de datos:
"""

y_test_array = np.array(list(y_test))
val_preds_hpm_reg_array = np.array(val_preds_hpm_reg)
hpm_df = pd.DataFrame({'Actual': y_test_array.flatten(), 'Predicted': val_preds_hpm_reg_array.flatten()})
hpm_df

"""Y podemos ver esta relación gráficamente con un diagrama de dispersión:"""

actual_values = y_test
plt.scatter(val_preds_hpm_reg, actual_values, alpha=.7,
            color='r') #alpha helps to show overlapping data
overlay = 'R^2 is: {}\nRMSE is: {}'.format(
                    (round(r2_score(y_test, val_preds_hpm_reg),4)),
                    (round(mean_squared_error(y_test, val_preds_hpm_reg))),4)
plt.annotate( s=overlay,xy=(5.5,2.5),size='x-large')
plt.xlabel('Predicted Price')
plt.ylabel('Actual Price')
plt.title('SHP Regression Model')
plt.show()

"""Si nuestros valores predichos fueran idénticos a los valores reales, este gráfico sería la línea recta `y = x` porque cada valor predicho x sería igual a cada valor real y.

##### Mejorando el modelo

Podemos intentar usar la Ridge Regularization para disminuir la influencia de características menos importantes. La regularización de ridges es un proceso que reduce los coeficientes de regresión de las características menos importantes.

Creamos una nueva instancia del modelo. El modelo de regularización de Ridge toma un parámetro, alfa, que controla la fuerza de la regularización.

Experimentaremos recorriendo algunos valores diferentes de alfa y veremos cómo esto cambia nuestros resultados.
"""

lr = linear_model.LinearRegression()

for i in range (-2, 3):
    alpha = 10**i
    rm = linear_model.Ridge(alpha=alpha)
    ridge_model = rm.fit(X_train, y_train)
    preds_ridge = ridge_model.predict(X_test)

    plt.scatter(preds_ridge, actual_values, alpha=.75, color='r')
    plt.xlabel('Predicted Price')
    plt.ylabel('Actual Price')
    plt.title('Ridge Regularization with alpha = {}'.format(alpha))
    overlay = 'R^2 is: {}\nRMSE is: {}'.format(
                   round(ridge_model.score(X_test, y_test), 4),
                    round(mean_squared_error(y_train, training_preds_hpm_reg),4))
    plt.annotate( s=overlay,xy=(5.5,2.5),size='x-large')
    plt.show()

"""Los dos modelos tienen casi la misma performance y ajustar las alfas no ha mejorado nuestro modelo

#### Modelo 2:  Gradient boosted decision trees

El refuerzo es una técnica de conjunto donde se agregan nuevos modelos para corregir los errores cometidos por los modelos existentes. Los modelos se agregan secuencialmente hasta que no se puedan realizar más mejoras. Un ejemplo popular es el algoritmo AdaBoost que pondera los puntos de datos que son difíciles de predecir.

El aumento de gradiente es un enfoque donde se crean nuevos modelos que predicen los residuos o errores de modelos anteriores y luego se suman para hacer la predicción final. Se llama aumento de gradiente porque utiliza un algoritmo de descenso de gradiente para minimizar la pérdida al agregar nuevos modelos.

XGBoost (eXtreme Gradient Boosting) es una implementación de árboles de decisión potenciados por gradiente diseñados para la velocidad y el rendimiento. Es un algoritmo muy popular que recientemente ha dominado el aprendizaje automático aplicado para datos estructurados o tabulares.

Este enfoque admite problemas de modelado predictivo de regresión y clasificación. Para obtener más información, vea aquí para escuchar a Tianqi Chen, el creador de la biblioteca XGBoost.

Es muy probable que este modelo proporcione la mejor precisión posible y una medida de la importancia de la característica en comparación con nuestra regresión hedónica (aparte de los posibles pequeños aumentos de precisión de la sintonización de hiperparámetros) debido al rendimiento superior de XGBoost.
"""

xgb_reg_start = time.time()

xgb_reg = xgb.XGBRegressor()
xgb_reg.fit(X_train, y_train)
training_preds_xgb_reg = xgb_reg.predict(X_train)
val_preds_xgb_reg = xgb_reg.predict(X_test)

xgb_reg_end = time.time()

print(f"Time taken to run: {round((xgb_reg_end - xgb_reg_start)/60,1)} minutes")
print("\nTraining MSE:", round(mean_squared_error(y_train, training_preds_xgb_reg),4))
print("Validation MSE:", round(mean_squared_error(y_test, val_preds_xgb_reg),4))
print("\nTraining r2:", round(r2_score(y_train, training_preds_xgb_reg),4))
print("Validation r2:", round(r2_score(y_test, val_preds_xgb_reg),4))

"""**Esto significa que nuestras características explican aproximadamente el 68% de la varianza en nuestra variable objetivo.**

##### Importancia de la característica

Importancia de la característica
Además de su rendimiento superior, una ventaja de usar conjuntos de métodos de árbol de decisión como el aumento de gradiente es que pueden proporcionar automáticamente estimaciones de la importancia de las características de un modelo predictivo entrenado.

En general, la importancia proporciona una puntuación que indica cuán útil o valiosa fue cada característica en la construcción de los árboles de decisión potenciados dentro del modelo. Cuanto más se utiliza un atributo para tomar decisiones clave con árboles de decisión, mayor es su importancia relativa.

Esta importancia se calcula explícitamente para cada atributo en el conjunto de datos, lo que permite clasificar y comparar los atributos entre sí.

La importancia se calcula para un solo árbol de decisión por la cantidad que cada punto de división de atributo mejora la medida de rendimiento, ponderada por el número de observaciones de las que es responsable el nodo. La medida del rendimiento puede ser la pureza (índice de Gini) utilizada para seleccionar los puntos de división u otra función de error más específica.

Las características importantes se promedian en todos los árboles de decisión dentro del modelo.
"""

ft_weights_xgb_reg = pd.DataFrame(xgb_reg.feature_importances_, columns=['weight'], index=X_train.columns)
ft_weights_xgb_reg.sort_values('weight', ascending=False, inplace=True)
ft_weights_xgb_reg.head(10)

# Plotting feature importances
plt.figure(figsize=(10,25))
plt.barh(ft_weights_xgb_reg.index, ft_weights_xgb_reg.weight, align='center') 
plt.title("Feature importances in the XGBoost model", fontsize=14)
plt.xlabel("Feature importance")
plt.margins(y=0.01)
plt.show()

"""Las características de ubicación cuando son corregidas por la distancia de features es la más relevante en nuestro modelo, o sea, pertenecer a un determinado vecindario aumenta el precio más que a otros y el 'Puntaje' (medida de accesibilidad) también muestra cierta importancia.
Una de las características interesantes es que el host_name es uno de las principales características determinantes del precio en nuestro modelo,  lo que no significa que un host que gestiona más propiedades dará como resultado tener precios más altos, y podría deberse a que los hosts experimentados establecen precios más altos. Además, podría ser que las grandes compañías de administración de Airbnb que tienen muchos listados tienden a administrar propiedades más caras que los anfitriones de listados únicos
"""



"""#### Selección del modelo final

En general, el modelo XGBoost (Modelo 2) es el modelo preferido, que funciona mejor que los modelos de regresión hedónica espacial y tan bueno como el primer modelo, pero es menos costoso computacionalmente. Posiblemente podría mejorarse aún más con el ajuste de hiperparámetros.

### Conclusions and Recommendations

El modelo con mejor desempeño fue capaz de predecir el 68.56% de la variación en el precio. Lo que significa que todavía tenemos un 31.44% restante sin explicación. Esto podría deberse a varias otras características que no forman parte de nuestro conjunto de datos o la necesidad de analizar nuestras características más de cerca.

Por ejemplo, dada la importancia de las revisiones de los clientes de la lista para determinar el precio, tal vez una mejor comprensión de las revisiones podría mejorar la predicción. Mediante el Análisis de opiniones, se puede asignar una puntuación entre -1 (opinión muy negativa) y 1 (opinión muy positiva) a cada revisión por propiedad de listado. Los puntajes se promedian en todas las revisiones asociadas con ese listado y los puntajes finales se pueden incluir como una nueva característica en el modelo (consulte [aquí] (https://arxiv.org/pdf/1907.12665.pdf) para ver un ejemplo )

Otra sugerencia es la inclusión de la calidad de imagen como característica. Utilizando el análisis profundo de Diferencia en Diferencia y aprendizaje supervisado en un conjunto de datos de panel de Airbnb, los investigadores descubrieron que las unidades con fotos verificadas (tomadas por los fotógrafos de Airbnb) generan ingresos adicionales por año en promedio ([ver aquí] (https: // poseidon01. ssrn.com/delivery.php?ID=204024105119026094069105008103096093034021070051045032025117103105027080085099113011048103001010014121023024085079004082003114107082070089028074094097015117086091125049040024103012093025001119016069070080070120120094005077005088015116003064009008121126&EXT=pdf)).

Se notó que la cercanía con puntos de interés fue más importante que la ubicación en sí (distancia punto central).Para los listados esto es importante dado que los hosts podrían tener mayores benefícios describiendo mejor los puntos de interés cercanos a su inmueble listado.
"""

